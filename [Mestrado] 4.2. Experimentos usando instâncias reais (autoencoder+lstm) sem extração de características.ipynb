{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Mestrado] 4.2. Experimentos usando instâncias reais (autoencoder+lstm) sem extração de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Artifício para alcular tempo total do notebook Jupyter\n",
    "from datetime import datetime \n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(0)\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "from math import ceil\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tsfresh').setLevel(logging.ERROR)\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = Path('./', 'data')\n",
    "random_state = 1\n",
    "np.random.seed(random_state)\n",
    "events_names = {0: 'Normal',\n",
    "                1: 'Aumento Abrupto de BSW',\n",
    "                2: 'Fechamento Espúrio de DHSV',\n",
    "                3: 'Intermitência Severa',\n",
    "                4: 'Instabilidade de Fluxo',\n",
    "                5: 'Perda Rápida de Produtividade',\n",
    "                6: 'Restrição Rápida em CKP',\n",
    "                7: 'Incrustação em CKP',\n",
    "                8: 'Hidrato em Linha de Produção'\n",
    "               }\n",
    "vars = ['P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        'P-JUS-CKGL',\n",
    "        'T-JUS-CKGL',\n",
    "        'QGL']\n",
    "columns = ['timestamp'] + vars + ['class'] \n",
    "normal_class_code = 0\n",
    "abnormal_classes_codes = [1, 2, 5, 6, 7, 8]\n",
    "sample_size = 3*60              # Nas observações = segundos\n",
    "min_normal_period_size = 20*60  # Nas observações = segundos\n",
    "split_range = 0.6               # Porcentagem de separação entre treino/teste\n",
    "max_samples_per_period = 15     # limitação por 'segurança'\n",
    "df_fc_p = MinimalFCParameters() # Ver documentação da biblioteca tsfresh\n",
    "df_fc_p.pop('sum_values')       # Remove feature inapropriada\n",
    "df_fc_p.pop('length')           # Remove feature inapropriada\n",
    "max_nan_percent = 0.1           # Para seleção de variáveis úteis\n",
    "std_vars_min = 0.01             # Para seleção de variáveis úteis\n",
    "clfs = {}                       # Dicionário para lista de classificadores a serem experimentados\n",
    "disable_progressbar = True      # Para menos saídas no notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def class_and_file_generator(data_path, real=False, simulated=False, drawn=False):\n",
    "    \"\"\"Gerador de lista contendo número da classe e caminho do arquivo de acordo com a fonte da instância.\"\"\"    \n",
    "    for class_path in data_path.iterdir():\n",
    "        if class_path.is_dir():\n",
    "            class_code = int(class_path.stem)\n",
    "            for instance_path in class_path.iterdir():\n",
    "                if (instance_path.suffix == '.csv'):\n",
    "                    if (simulated and instance_path.stem.startswith('SIMULATED')) or \\\n",
    "                       (drawn and instance_path.stem.startswith('DRAWN')) or \\\n",
    "                       (real and (not instance_path.stem.startswith('SIMULATED')) and \\\n",
    "                       (not instance_path.stem.startswith('DRAWN'))):\n",
    "                        yield class_code, instance_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_instance(instance_path):\n",
    "    \"\"\"Função que carrega cada instância individualmente\"\"\"\n",
    "    try:\n",
    "        well, instance_id = instance_path.stem.split('_')\n",
    "        df = pd.read_csv(instance_path, sep=',', header=0)\n",
    "        assert (df.columns == columns).all(), \\\n",
    "            f'Colunas inválidas no arquivo {str(instance_path)}: {str(df.columns.tolist())}'\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f'Erro ao ler arquivo {instance_path}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_samples(df, class_code):\n",
    "    # Obtém os rótulos das observações e seu conjunto inequívoco\n",
    "    ols = list(df['class'])\n",
    "    set_ols = set()\n",
    "    for ol in ols:\n",
    "        if ol in set_ols or np.isnan(ol):\n",
    "            continue\n",
    "        set_ols.add(int(ol))       \n",
    "    \n",
    "    # Descarta os rótulos das observações e substitui todos os nan por 0\n",
    "    # (requisito da biblioteca tsfresh)\n",
    "    df_vars = df.drop('class', axis=1).fillna(0)  \n",
    "    \n",
    "    # Inicializa objetos que serão retornados\n",
    "    df_samples_train = pd.DataFrame()\n",
    "    df_samples_test = pd.DataFrame()\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "            \n",
    "    # Descubre o número máximo de amostras em períodos normais, transitórios e em regime\n",
    "    # Obtém índices (primeiro e último) sem sobreposição com outros períodos\n",
    "    f_idx = ols.index(normal_class_code)\n",
    "    l_idx = len(ols)-1-ols[::-1].index(normal_class_code)\n",
    "\n",
    "    # Define o número inicial de amostras para o período normal\n",
    "    max_samples_normal = l_idx-f_idx+1-sample_size\n",
    "    if (max_samples_normal) > 0:      \n",
    "        num_normal_samples = min(max_samples_per_period, max_samples_normal)\n",
    "        num_train_samples = int(split_range*num_normal_samples)\n",
    "        num_test_samples = num_normal_samples - num_train_samples    \n",
    "    else:\n",
    "        num_train_samples = 0\n",
    "        num_test_samples = 0\n",
    "    \n",
    "    # Define o número máximo de amostras por período transitório\n",
    "    transient_code = class_code + 100    \n",
    "    if transient_code in set_ols:\n",
    "        # Obtém índices (primeiro e último) com possível sobreposição\n",
    "        # no início do período\n",
    "        f_idx = ols.index(transient_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(transient_code)        \n",
    "        max_transient_samples = l_idx-f_idx+1-sample_size\n",
    "    else:\n",
    "        max_transient_samples = 0            \n",
    "\n",
    "    # Define o número máximo de amostras no período de regime\n",
    "    if class_code in set_ols:\n",
    "        # Obtém índices (primeiro e último) com possível sobreposição \n",
    "        # no início ou fim do período\n",
    "        f_idx = ols.index(class_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(class_code)\n",
    "        if l_idx+(sample_size-1) < len(ols)-1:\n",
    "            l_idx = l_idx+(sample_size-1) \n",
    "        else:\n",
    "            l_idx = len(ols)-1\n",
    "        max_in_regime_samples = l_idx-f_idx+1-sample_size\n",
    "    else:\n",
    "        max_in_regime_samples = 0   \n",
    "        \n",
    "    # Descubre o número adequado de amostras em períodos normais, transitórios e em regime\n",
    "    num_transient_samples = ceil(num_test_samples/2)\n",
    "    num_in_regime_samples = num_test_samples - num_transient_samples\n",
    "    if (max_transient_samples >= num_transient_samples) and \\\n",
    "       (max_in_regime_samples < num_in_regime_samples):\n",
    "        num_in_regime_samples = max_in_regime_samples        \n",
    "        num_transient_samples = min(num_test_samples-num_in_regime_samples, max_transient_samples)\n",
    "    elif (max_transient_samples < num_transient_samples) and \\\n",
    "         (max_in_regime_samples >= num_in_regime_samples):\n",
    "        num_transient_samples = max_transient_samples        \n",
    "        num_in_regime_samples = min(num_test_samples-num_transient_samples, max_in_regime_samples)\n",
    "    elif (max_transient_samples < num_transient_samples) and \\\n",
    "         (max_in_regime_samples < num_in_regime_samples):\n",
    "        num_transient_samples = max_transient_samples\n",
    "        num_in_regime_samples = max_in_regime_samples\n",
    "        num_test_samples = num_transient_samples+num_in_regime_samples\n",
    "    \n",
    "    # Extrai amostras do período normal para treinamento e teste\n",
    "    # Obtém índices (primeiro e último) sem sobreposição com outros períodos\n",
    "    f_idx = ols.index(normal_class_code)\n",
    "    l_idx = len(ols)-1-ols[::-1].index(normal_class_code)\n",
    "    \n",
    "    # Define a etapa correta e extrai amostras\n",
    "    if (num_normal_samples) > 0:  \n",
    "        if num_normal_samples == max_samples_normal:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_samples_normal-1) // (max_samples_per_period-1)\n",
    "        step_wanted = sample_size\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Extrai amostras para treinamento\n",
    "        sample_id = 0\n",
    "        for idx in range(num_train_samples):\n",
    "            f_idx_c = l_idx-sample_size+1-(num_normal_samples-1-idx)*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_train = df_samples_train.append(df_sample)\n",
    "            y_train.append(normal_class_code)\n",
    "            sample_id += 1\n",
    "    \n",
    "        # Extrai amostras para teste\n",
    "        sample_id = 0\n",
    "        for idx in range(num_train_samples, num_train_samples+num_test_samples):\n",
    "            f_idx_c = l_idx-sample_size+1-(num_normal_samples-1-idx)*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(normal_class_code)\n",
    "            sample_id += 1\n",
    "\n",
    "    # Extrai amostras do período transitório (se existir) para teste\n",
    "    if (num_transient_samples) > 0:    \n",
    "        # Define a etapa correta e extrai amostras\n",
    "        if num_transient_samples == max_transient_samples:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_transient_samples-1) // (max_samples_per_period-1)\n",
    "        step_wanted = np.inf\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Obtém índices (primeiro e último) com possível sobreposição no início deste período\n",
    "        f_idx = ols.index(transient_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(transient_code) \n",
    "\n",
    "        # Extrai amostras\n",
    "        for idx in range(num_transient_samples):\n",
    "            f_idx_c = f_idx+idx*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(transient_code)\n",
    "            sample_id += 1\n",
    "            \n",
    "    # Extrai amostras do período em regime (se existir) para teste\n",
    "    if (num_in_regime_samples) > 0:     \n",
    "        # Define a etapa correta e extrai amostras\n",
    "        if num_in_regime_samples == max_in_regime_samples:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_in_regime_samples-1) // (max_samples_per_period-1)\n",
    "        step_wanted = sample_size\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Obtém índices (primeiro e último) com possível sobreposição \n",
    "        # no início ou no final deste período\n",
    "        f_idx = ols.index(class_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(class_code)\n",
    "        if l_idx+(sample_size-1) < len(ols)-1:\n",
    "            l_idx = l_idx+(sample_size-1) \n",
    "        else:\n",
    "            l_idx = len(ols)-1\n",
    "\n",
    "        # Extrai amostras\n",
    "        for idx in range(num_in_regime_samples):\n",
    "            f_idx_c = f_idx+idx*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(class_code)\n",
    "            sample_id += 1\n",
    "\n",
    "    return df_samples_train, y_train, df_samples_test, y_test              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gets all selected instances but maintains only those with any type of undesirable event\n",
    "real_instances = pd.DataFrame(class_and_file_generator(data_path, \n",
    "                                                       real=True,\n",
    "                                                       simulated=False, \n",
    "                                                       drawn=False),\n",
    "                              columns=['class_code', 'instance_path'])\n",
    "real_instances = real_instances.loc[real_instances.iloc[:,0].isin(abnormal_classes_codes)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instância 1: data\\1\\WELL-00001_20140124213136.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (959)\n",
      "\n",
      "Instância 2: data\\1\\WELL-00002_20140126200050.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (1138)\n",
      "\n",
      "Instância 3: data\\1\\WELL-00006_20170801063614.csv\n",
      "Instância 4: data\\1\\WELL-00006_20170802123000.csv\n",
      "Instância 5: data\\1\\WELL-00006_20180618060245.csv\n",
      "Instância 6: data\\2\\WELL-00002_20131104014101.csv\n",
      "Instância 7: data\\2\\WELL-00003_20141122214325.csv\n",
      "Instância 8: data\\2\\WELL-00003_20170728150240.csv\n",
      "Instância 9: data\\2\\WELL-00003_20180206182917.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (586)\n",
      "\n",
      "Instância 10: data\\2\\WELL-00009_20170313160804.csv\n",
      "Instância 11: data\\2\\WELL-00010_20171218200131.csv\n",
      "Instância 12: data\\2\\WELL-00011_20140515110134.csv\n",
      "Instância 13: data\\2\\WELL-00011_20140530100015.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (482)\n",
      "\n",
      "Instância 14: data\\2\\WELL-00011_20140606230115.csv\n",
      "Instância 15: data\\2\\WELL-00011_20140720120102.csv\n",
      "Instância 16: data\\2\\WELL-00011_20140726180015.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (900)\n",
      "\n",
      "Instância 17: data\\2\\WELL-00011_20140824000118.csv\n",
      "Instância 18: data\\2\\WELL-00011_20140916060300.csv\n",
      "Instância 19: data\\2\\WELL-00011_20140921200031.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (695)\n",
      "\n",
      "Instância 20: data\\2\\WELL-00011_20140928100056.csv\n",
      "Instância 21: data\\2\\WELL-00011_20140929170028.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (975)\n",
      "\n",
      "Instância 22: data\\2\\WELL-00011_20140929220121.csv\n",
      "Instância 23: data\\2\\WELL-00011_20141005170056.csv\n",
      "Instância 24: data\\2\\WELL-00011_20141006160121.csv\n",
      "Instância 25: data\\2\\WELL-00012_20170320033022.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (773)\n",
      "\n",
      "Instância 26: data\\2\\WELL-00012_20170320143144.csv\n",
      "Instância 27: data\\2\\WELL-00013_20170329020229.csv\n",
      "Instância 28: data\\5\\WELL-00015_20170620160349.csv\n",
      "Instância 29: data\\5\\WELL-00015_20171013140047.csv\n",
      "Instância 30: data\\5\\WELL-00016_20180405020345.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (1145)\n",
      "\n",
      "Instância 31: data\\5\\WELL-00016_20180426142005.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (321)\n",
      "\n",
      "Instância 32: data\\5\\WELL-00016_20180426145108.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (376)\n",
      "\n",
      "Instância 33: data\\5\\WELL-00016_20180517222322.csv\n",
      "Instância 34: data\\5\\WELL-00017_20140314180000.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (0)\n",
      "\n",
      "Instância 35: data\\5\\WELL-00017_20140317151743.csv\n",
      "Instância 36: data\\5\\WELL-00017_20140318023141.csv\n",
      "Instância 37: data\\5\\WELL-00017_20140318160220.csv\n",
      "Instância 38: data\\5\\WELL-00017_20140319040453.csv\n",
      "Instância 39: data\\5\\WELL-00017_20140319141450.csv\n",
      "Instância 40: data\\6\\WELL-00002_20140212170333.csv\n",
      "Instância 41: data\\6\\WELL-00002_20140301151700.csv\n",
      "Instância 42: data\\6\\WELL-00002_20140325170304.csv\n",
      "Instância 43: data\\6\\WELL-00004_20171031181509.csv\n",
      "Instância 44: data\\6\\WELL-00004_20171031193025.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (414)\n",
      "\n",
      "Instância 45: data\\6\\WELL-00004_20171031200059.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (845)\n",
      "\n",
      "Instância 46: data\\7\\WELL-00001_20170226220309.csv\n",
      "Instância 47: data\\7\\WELL-00006_20180618110721.csv\n",
      "Instância 48: data\\7\\WELL-00006_20180620181348.csv\n",
      "Instância 49: data\\7\\WELL-00018_20180611040207.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (0)\n",
      "\n",
      "Instância 50: data\\8\\WELL-00019_20170301182317.csv\n",
      "Instância 51: data\\8\\WELL-00020_20120410192326.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (173)\n",
      "\n",
      "Instância 52: data\\8\\WELL-00021_20170509013517.csv\n"
     ]
    }
   ],
   "source": [
    "# For each real instance with any type of undesirable event\n",
    "scores = pd.DataFrame()\n",
    "ignored_instances = 0\n",
    "used_instances = 0\n",
    "\n",
    "for i, row in real_instances.iterrows():\n",
    "    # Loads the current instance\n",
    "    class_code, instance_path = row\n",
    "    print(f'Instância {i+1}: {instance_path}')\n",
    "    df = load_instance(instance_path)\n",
    "    \n",
    "    # Ignores instances without sufficient normal periods\n",
    "    normal_period_size = (df['class']==float(normal_class_code)).sum()\n",
    "    if normal_period_size < min_normal_period_size:\n",
    "        ignored_instances += 1\n",
    "        print(f'\\tignorado porque normal_period_size é insuficiente para treinamento ({normal_period_size})\\n')\n",
    "        continue\n",
    "    used_instances += 1\n",
    "        \n",
    "    # Extracts samples from the current real instance\n",
    "    ret = extract_samples(df, class_code)\n",
    "    df_samples_train, y_train, df_samples_test, y_test = ret\n",
    "\n",
    "    # Changes types of the labels (tsfresh's requirement)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # We want binary classification: 1 stands for inliers and -1 for outliers/anomalies\n",
    "    y_train[y_train!=normal_class_code] = -1\n",
    "    y_train[y_train==normal_class_code] = 1   \n",
    "    y_test[y_test!=normal_class_code] = -1\n",
    "    y_test[y_test==normal_class_code] = 1\n",
    "    \n",
    "    # Drops the bad vars\n",
    "    good_vars = np.isnan(df_samples_train[vars]).mean(0) <= max_nan_percent\n",
    "    std_vars = np.nanstd(df_samples_train[vars], 0)\n",
    "    good_vars &= (std_vars > std_vars_min)    \n",
    "    good_vars = list(good_vars.index[good_vars])\n",
    "    bad_vars = list(set(vars)-set(good_vars))\n",
    "    df_samples_train.drop(columns=bad_vars, inplace=True, errors='ignore')\n",
    "    df_samples_test.drop(columns=bad_vars, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Normalizes the samples (zero mean and unit variance)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    df_samples_train[good_vars] = scaler.fit_transform(df_samples_train[good_vars]).astype('float32')\n",
    "    df_samples_test[good_vars] = scaler.transform(df_samples_test[good_vars]).astype('float32')\n",
    "    \n",
    "    # Remove a coluna timestamp (não é usada como entrada nos classificadores)\n",
    "    df_samples_train = df_samples_train.drop('timestamp', 1)\n",
    "    df_samples_test = df_samples_test.drop('timestamp', 1)\n",
    "    \n",
    "    # Rearranja o dataframe df_samples_train para que as variáveis estejam em linha\n",
    "    df_samples_train_ajustado = (\n",
    "        df_samples_train.pivot_table(values=None, \n",
    "                                     index='id', \n",
    "                                     columns=df_samples_train.groupby('id').cumcount() + 1, \n",
    "                                     aggfunc='first')\n",
    "        .reset_index()\n",
    "        )\n",
    "        \n",
    "    # Rearranja o dataframe df_samples_test para que as variáveis estejam em linha\n",
    "    df_samples_test_ajustado = (\n",
    "        df_samples_test.pivot_table(values=None, \n",
    "                                     index='id', \n",
    "                                     columns=df_samples_test.groupby('id').cumcount() + 1, \n",
    "                                     aggfunc='first')\n",
    "        .reset_index()\n",
    "        )\n",
    "        \n",
    "    X_train = df_samples_train_ajustado\n",
    "    X_test = df_samples_test_ajustado\n",
    "    \n",
    "    X_train.drop('id',  axis='columns', inplace=True)\n",
    "    X_test.drop('id',  axis='columns', inplace=True)\n",
    "    \n",
    "    X_train.reset_index(inplace=True, drop=True)\n",
    "    X_test.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Reshape dos dados para formato de entrada da rede LSTM\n",
    "    # que é um array 3D no formato (observações, timesteps, features)\n",
    "    X_train = X_train.values.reshape(X_train.shape[0], 180, X_train.shape[1] // 180, order='F')\n",
    "    X_test = X_test.values.reshape(X_test.shape[0], 180, X_test.shape[1] // 180, order='F')\n",
    "\n",
    "    # LISTA DE CLASSIFICADORES A SEREM EXPERIMENTADOS\n",
    "    \n",
    "    # Aplicação do classificador com parâmetros obtidos nas instancias simuladas\n",
    "\n",
    "    grid_params = {\n",
    "        'batch_size': [4],\n",
    "        'epochs': [10],\n",
    "        'lstm_units': [16],\n",
    "        'dropout': [0],\n",
    "    }\n",
    "\n",
    "    for params in ParameterGrid(grid_params):\n",
    "\n",
    "        #print(f'PARÂMETROS: {params}')\n",
    "\n",
    "        # Define o modelo de rede neural\n",
    "        lstm_encoder = Sequential([\n",
    "            LSTM(int(params['lstm_units']), return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            Dropout(rate=params['dropout']),\n",
    "            LSTM(int(params['lstm_units']/2), return_sequences=False),\n",
    "            Dropout(rate=params['dropout']),\n",
    "        ], name='encoder')\n",
    "        lstm_decoder = Sequential([\n",
    "            RepeatVector(X_train.shape[1]),\n",
    "            LSTM(int(params['lstm_units']/2), return_sequences=True),\n",
    "            Dropout(rate=params['dropout']),\n",
    "            LSTM(int(params['lstm_units']), return_sequences=True),\n",
    "            Dropout(rate=params['dropout']),\n",
    "            TimeDistributed(Dense(X_train.shape[2]))    \n",
    "        ], name='decoder')\n",
    "        model = Sequential([lstm_encoder, lstm_decoder])\n",
    "        model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "        # Treina o modelo\n",
    "        t0 = time()\n",
    "        history = model.fit(X_train, X_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n",
    "        t_train = time() - t0\n",
    "\n",
    "        # Gráfico de train e validation loss\n",
    "        #fig1, ax1 = plt.subplots()\n",
    "        #ax1.set_title(f'{params}')\n",
    "        #ax1.plot(history.history['loss'], label=f'Training loss {params}')\n",
    "        #ax1.set_xlabel('epochs')\n",
    "        #ax1.set_ylabel('mae')\n",
    "        #ax1.legend()\n",
    "        #plt.show()\n",
    "        \n",
    "        t0 = time()\n",
    "        trainPredict = model.predict(X_train)\n",
    "        trainMAE = np.mean(np.mean(np.abs(trainPredict - X_train), axis=1), axis=1)\n",
    "        mean_trainMAE = np.mean(trainMAE)\n",
    "        #print(f'mean_trainMAE: {mean_trainMAE}')\n",
    "\n",
    "        #fig2, ax2 = plt.subplots()\n",
    "        #ax2.set_title(f'trainMAE')\n",
    "        #ax2.hist(trainMAE)\n",
    "        #plt.show()\n",
    "\n",
    "        max_trainMAE = np.max(trainMAE)\n",
    "        #print(f'max_trainMAE: \\t {max_trainMAE}')\n",
    "\n",
    "        testPredict = model.predict(X_test)\n",
    "        testMAE = np.mean(np.mean(np.abs(testPredict - X_test), axis=1), axis=1)\n",
    "\n",
    "        #fig3, ax3 = plt.subplots()\n",
    "        #ax3.set_title(f'testMAE')\n",
    "        #Para o histograma, todos os valores maiores que 10 setei como 10 para melhor visualização\n",
    "        #testMAE_visualizacao = testMAE\n",
    "        #testMAE_visualizacao[testMAE_visualizacao > 10] = 10\n",
    "        #ax3.hist(testMAE_visualizacao)\n",
    "        #plt.show()\n",
    "\n",
    "        # Será considerada anomalia aquela observação cujo erro de reconstrução seja \n",
    "        # acima de um valor especificado como gatilho (usado max_trainMAE)\n",
    "        y_pred = pd.DataFrame((testMAE >= max_trainMAE)).replace({True:-1,False:1})\n",
    "        \n",
    "        t_test = time() - t0\n",
    "\n",
    "        # Calcula as metricas de desempenho\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "\n",
    "        #print(f'F1: \\t {f1}')\n",
    "\n",
    "        scores = scores.append({'CLASSIFICADOR': 'AutoEncoder LSTM', \n",
    "                                'PRECISAO': p,\n",
    "                                'REVOGACAO': r,\n",
    "                                'F1': f1,\n",
    "                                'TREINAMENTO [s]': t_train, \n",
    "                                'TESTE [s] ': t_test}, ignore_index=True) \n",
    "        \n",
    "        #print('\\n#################################################################\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados obtidos com os métodos implementados são apresentados abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de instâncias utilizadas: 36\n",
      "Número de instâncias ignoradas: 16\n"
     ]
    }
   ],
   "source": [
    "print(f'Número de instâncias utilizadas: {used_instances}')\n",
    "print(f'Número de instâncias ignoradas: {ignored_instances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.to_csv(r'./results/4-2_anomaly_detection_scores_reais.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas em formato tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As tabelas a seguir apresentam as médias e o desvio padrão das métricas, respectivamente. Ambos são ordenados pela medida-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISAO</th>\n",
       "      <th>REVOGACAO</th>\n",
       "      <th>F1</th>\n",
       "      <th>TREINAMENTO [s]</th>\n",
       "      <th>TESTE [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIFICADOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AutoEncoder LSTM</th>\n",
       "      <td>0.627315</td>\n",
       "      <td>0.627315</td>\n",
       "      <td>0.627315</td>\n",
       "      <td>9.693188</td>\n",
       "      <td>1.448803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PRECISAO  REVOGACAO        F1  TREINAMENTO [s]  TESTE [s] \n",
       "CLASSIFICADOR                                                               \n",
       "AutoEncoder LSTM  0.627315   0.627315  0.627315         9.693188    1.448803"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Médias\n",
    "mean_score_table = scores.groupby('CLASSIFICADOR').mean().sort_values(by=['F1'], ascending=False)\n",
    "mean_score_table.to_csv(r'./results/4-2_anomaly_detection_scores_medias_reais.csv')\n",
    "mean_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISAO</th>\n",
       "      <th>REVOGACAO</th>\n",
       "      <th>F1</th>\n",
       "      <th>TREINAMENTO [s]</th>\n",
       "      <th>TESTE [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIFICADOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AutoEncoder LSTM</th>\n",
       "      <td>0.179821</td>\n",
       "      <td>0.179821</td>\n",
       "      <td>0.179821</td>\n",
       "      <td>0.76317</td>\n",
       "      <td>0.293154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PRECISAO  REVOGACAO        F1  TREINAMENTO [s]  TESTE [s] \n",
       "CLASSIFICADOR                                                               \n",
       "AutoEncoder LSTM  0.179821   0.179821  0.179821          0.76317    0.293154"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desvios Padrão\n",
    "std_score_table = scores.groupby('CLASSIFICADOR').std().sort_values(by=['F1'], ascending=True)\n",
    "std_score_table.to_csv(r'./results/4-2_anomaly_detection_scores_desvios_padrao_reais.csv')\n",
    "std_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execução (hh:mm:ss.ms): 0:08:18.566929\n"
     ]
    }
   ],
   "source": [
    "# Calcular tempo total do notebook Jupyter\n",
    "print(f'Tempo total de execução (hh:mm:ss.ms): {datetime.now() - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Sumário",
   "title_sidebar": "Sumário",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
