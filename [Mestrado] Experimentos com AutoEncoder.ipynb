{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos com AutoEncoder\n",
    "## Detecção de anomalias em poços produtores de petróleo usando aprendizado de máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Principais Referências_**\n",
    "  \n",
    "- **A Realistic and Public Dataset with Rare Undesirable Real Events in Oil Wells** ([link](https://doi.org/10.1016/j.petrol.2019.106223)).\n",
    "- **Github de referência do _benchmark_ proposto por Vargas (2019)** ([link](https://github.com/ricardovvargas/3w_dataset))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regras do benchmark proposto por Vargas (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apenas instâncias reais com anomalias de tipos que têm períodos normais maiores ou iguais a vinte minutos foram utilizadas;\n",
    "\n",
    "- Múltiplas rodadas de treinamento e validação realizadas, sendo o número de rodadas igual ao número de instâncias. Em cada rodada, amostras utilizadas para treinamento ou validação extraídas de apenas uma instância. Parte das amostras de normalidade utilizadas no treinamento (60%) e a outra parte para teste (40%). As amostras de anomalias somente devem ser utilizadas apenas teste, sendo, portanto, uma técnica de treinamento de classe única. O conjunto de teste deve ser composto pelo mesmo número de amostras de cada classe (normalidade e anormalidade);\n",
    "\n",
    "- Em cada rodada, precisão, revogação e medida F1 devem ser computadas (valor médio e desvio padrão de cada métrica), sendo o valor médio da medida F1 considerado a principal métrica de desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Outras definições adotadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uma estratégia de amostragem específica com janela deslizante foi usada para cada tipo de período. Em períodos normais, as primeiras observações são usadas para treinamento e as últimas são usadas para testes. Em períodos transientes, procura-se usar observações como um todo (apenas para teste). Em períodos de regime, as primeiras observações são privilegiadas (somente para teste);\n",
    "\n",
    "- Antes de cada rodada de treinamento e teste:\n",
    "    - As amostras utilizadas (não as instâncias) são adequadamente normalizadas com o z-score;\n",
    "    - As variáveis de amostras (não as instâncias) usadas para treinamento que possuem um número de NaNs acima de um limite ou que têm um desvio padrão abaixo de um outro limite são descartadas.\n",
    "\n",
    "- Todos os random_state necessários são atribuídos a uma constante para que os resultados sejam reproduzíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bibliotecas e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Artifício para alcular tempo total do notebook Jupyter\n",
    "from datetime import datetime \n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('stac')\n",
    "import nonparametric_tests as stac\n",
    "from math import ceil\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Using Auto Encoder with Outlier Detection\"\"\"\n",
    "# Author: Yue Zhao <zhaoy@cmu.edu>\n",
    "# License: BSD 2 clause\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "\n",
    "\n",
    "class AutoEncoder():\n",
    "    \"\"\"Auto Encoder (AE) is a type of neural networks for learning useful data\n",
    "    representations unsupervisedly. Similar to PCA, AE could be used to\n",
    "    detect outlying objects in the data by calculating the reconstruction\n",
    "    errors. See :cite:`aggarwal2015outlier` Chapter 3 for details.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_neurons : list, optional (default=[64, 32, 32, 64])\n",
    "        The number of neurons per hidden layers.\n",
    "\n",
    "    hidden_activation : str, optional (default='relu')\n",
    "        Activation function to use for hidden layers.\n",
    "        All hidden layers are forced to use the same type of activation.\n",
    "        See https://keras.io/activations/\n",
    "\n",
    "    output_activation : str, optional (default='sigmoid')\n",
    "        Activation function to use for output layer.\n",
    "        See https://keras.io/activations/\n",
    "\n",
    "    loss : str or obj, optional (default=keras.losses.mean_squared_error)\n",
    "        String (name of objective function) or objective function.\n",
    "        See https://keras.io/losses/\n",
    "\n",
    "    optimizer : str, optional (default='adam')\n",
    "        String (name of optimizer) or optimizer instance.\n",
    "        See https://keras.io/optimizers/\n",
    "\n",
    "    epochs : int, optional (default=100)\n",
    "        Number of epochs to train the model.\n",
    "\n",
    "    batch_size : int, optional (default=32)\n",
    "        Number of samples per gradient update.\n",
    "\n",
    "    dropout_rate : float in (0., 1), optional (default=0.2)\n",
    "        The dropout to be used across all layers.\n",
    "\n",
    "    l2_regularizer : float in (0., 1), optional (default=0.1)\n",
    "        The regularization strength of activity_regularizer\n",
    "        applied on each layer. By default, l2 regularizer is used. See\n",
    "        https://keras.io/regularizers/\n",
    "\n",
    "    validation_size : float in (0., 1), optional (default=0.1)\n",
    "        The percentage of data to be used for validation.\n",
    "\n",
    "    preprocessing : bool, optional (default=False)\n",
    "        If True, apply standardization on the data.\n",
    "\n",
    "    verbose : int, optional (default=1)\n",
    "        Verbosity mode.\n",
    "\n",
    "        - 0 = silent\n",
    "        - 1 = progress bar\n",
    "        - 2 = one line per epoch.\n",
    "\n",
    "        For verbosity >= 1, model summary may be printed.\n",
    "\n",
    "    random_state : random_state: int, RandomState instance or None, optional\n",
    "        (default=None)\n",
    "        If int, random_state is the seed used by the random\n",
    "        number generator; If RandomState instance, random_state is the random\n",
    "        number generator; If None, the random number generator is the\n",
    "        RandomState instance used by `np.random`.\n",
    "\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        The amount of contamination of the data set, i.e.\n",
    "        the proportion of outliers in the data set. When fitting this is used\n",
    "        to define the threshold on the decision function.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    encoding_dim_ : int\n",
    "        The number of neurons in the encoding layer.\n",
    "\n",
    "    compression_rate_ : float\n",
    "        The ratio between the original feature and\n",
    "        the number of neurons in the encoding layer.\n",
    "\n",
    "    model_ : Keras Object\n",
    "        The underlying AutoEncoder in Keras.\n",
    "\n",
    "    history_: Keras Object\n",
    "        The AutoEncoder training history.\n",
    "\n",
    "    decision_scores_ : numpy array of shape (n_samples,)\n",
    "        The outlier scores of the training data.\n",
    "        The higher, the more abnormal. Outliers tend to have higher\n",
    "        scores. This value is available once the detector is\n",
    "        fitted.\n",
    "\n",
    "    threshold_ : float\n",
    "        The threshold is based on ``contamination``. It is the\n",
    "        ``n_samples * contamination`` most abnormal samples in\n",
    "        ``decision_scores_``. The threshold is calculated for generating\n",
    "        binary outlier labels.\n",
    "\n",
    "    labels_ : int, either +1 or -1\n",
    "        The binary labels of the training data. 0 stands for inliers\n",
    "        and 1 for outliers/anomalies. It is generated by applying\n",
    "        ``threshold_`` on ``decision_scores_``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_neurons=[64, 32, 32, 64],\n",
    "                 hidden_activation='relu', output_activation='sigmoid',\n",
    "                 loss=mean_squared_error, optimizer='adam',\n",
    "                 epochs=100, batch_size=32, dropout_rate=0.2,\n",
    "                 l2_regularizer=0.1, validation_size=0.1, preprocessing=False,\n",
    "                 verbose=1, random_state=None, contamination=0.1):\n",
    "        self.contamination = contamination\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_regularizer = l2_regularizer\n",
    "        self.validation_size = validation_size\n",
    "        self.preprocessing = preprocessing\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "        # Verify the network design is valid\n",
    "        if not self.hidden_neurons == self.hidden_neurons[::-1]:\n",
    "            print(self.hidden_neurons)\n",
    "            raise ValueError(\"Hidden units should be symmetric\")\n",
    "\n",
    "        self.hidden_neurons_ = self.hidden_neurons\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        # Input layer\n",
    "        model.add(Dense(\n",
    "            self.hidden_neurons_[0], activation=self.hidden_activation,\n",
    "            input_shape=(self.n_features_,),\n",
    "            activity_regularizer=l2(self.l2_regularizer)))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "\n",
    "        # Additional layers\n",
    "        for i, hidden_neurons in enumerate(self.hidden_neurons_, 1):\n",
    "            model.add(Dense(\n",
    "                hidden_neurons,\n",
    "                activation=self.hidden_activation,\n",
    "                activity_regularizer=l2(self.l2_regularizer)))\n",
    "            model.add(Dropout(self.dropout_rate))\n",
    "\n",
    "        # Output layers\n",
    "        model.add(Dense(self.n_features_, activation=self.output_activation,\n",
    "                        activity_regularizer=l2(self.l2_regularizer)))\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "        if self.verbose >= 1:\n",
    "            print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit detector. y is ignored in unsupervised methods.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "\n",
    "        y : Ignored\n",
    "            Not used, present for API consistency by convention.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # validate inputs X and y (optional)\n",
    "        X = check_array(X)\n",
    "\n",
    "        # Verify and construct the hidden units\n",
    "        self.n_samples_, self.n_features_ = X.shape[0], X.shape[1]\n",
    "\n",
    "        # Standardize data for better performance\n",
    "        if self.preprocessing:\n",
    "            self.scaler_ = StandardScaler()\n",
    "            X_norm = self.scaler_.fit_transform(X)\n",
    "        else:\n",
    "            X_norm = np.copy(X)\n",
    "\n",
    "        # Shuffle the data for validation as Keras do not shuffling for\n",
    "        # Validation Split\n",
    "        np.random.shuffle(X_norm)\n",
    "\n",
    "        # Validate and complete the number of hidden neurons\n",
    "        if np.min(self.hidden_neurons) > self.n_features_:\n",
    "            raise ValueError(\"The number of neurons should not exceed \"\n",
    "                             \"the number of features\")\n",
    "        self.hidden_neurons_.insert(0, self.n_features_)\n",
    "\n",
    "        # Calculate the dimension of the encoding layer & compression rate\n",
    "        self.encoding_dim_ = np.median(self.hidden_neurons)\n",
    "        self.compression_rate_ = self.n_features_ // self.encoding_dim_\n",
    "\n",
    "        # Build AE model & fit with X\n",
    "        self.model_ = self._build_model()\n",
    "        self.history_ = self.model_.fit(X_norm, X_norm,\n",
    "                                        epochs=self.epochs,\n",
    "                                        batch_size=self.batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        validation_split=self.validation_size,\n",
    "                                        verbose=self.verbose).history\n",
    "        # Reverse the operation for consistency\n",
    "        self.hidden_neurons_.pop(0)\n",
    "        # Predict on X itself and calculate the reconstruction error as\n",
    "        # the outlier scores. Noted X_norm was shuffled has to recreate\n",
    "        if self.preprocessing:\n",
    "            X_norm = self.scaler_.transform(X)\n",
    "        else:\n",
    "            X_norm = np.copy(X)\n",
    "\n",
    "        pred_scores = self.model_.predict(X_norm)\n",
    "        self.decision_scores_ = np.sqrt(np.sum(np.square(pred_scores - X_norm), axis=1)).ravel()\n",
    "        \n",
    "        self._process_decision_scores()\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Predict raw anomaly score of X using the fitted detector.\n",
    "\n",
    "        The anomaly score of an input sample is computed based on different\n",
    "        detector algorithms. For consistency, outliers are assigned with\n",
    "        larger anomaly scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The training input samples. Sparse matrices are accepted only\n",
    "            if they are supported by the base estimator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        anomaly_scores : numpy array of shape (n_samples,)\n",
    "            The anomaly score of the input samples.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['model_', 'history_'])\n",
    "        X = check_array(X)\n",
    "\n",
    "        if self.preprocessing:\n",
    "            X_norm = self.scaler_.transform(X)\n",
    "        else:\n",
    "            X_norm = np.copy(X)\n",
    "\n",
    "        # Predict on X and return the reconstruction errors\n",
    "        pred_scores = self.model_.predict(X_norm)\n",
    "        return np.sqrt(np.sum(np.square(pred_scores - X_norm), axis=1)).ravel()\n",
    "\n",
    "    \n",
    "    def _process_decision_scores(self, threshold=None):\n",
    "        \"\"\"Internal function to calculate key attributes:\n",
    "\n",
    "        - threshold_: used to decide the binary label\n",
    "        - labels_: binary labels of training data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        self.threshold_ = np.percentile(self.decision_scores_,\n",
    "                                     100 * (1 - self.contamination))\n",
    "        \n",
    "        self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
    "            'int').ravel()\n",
    "\n",
    "        # calculate for predict_proba()\n",
    "        self._mu = np.mean(self.decision_scores_)\n",
    "        self._sigma = np.std(self.decision_scores_)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict if a particular sample is an outlier or not.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        is_inlier : ndarray of shape (n_samples,)\n",
    "        Returns -1 for anomalies/outliers and +1 for inliers.\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])\n",
    "        \n",
    "        is_inlier = np.ones(X.shape[0], dtype=int)\n",
    "        is_inlier[self.decision_function(X) < self.threshold_] = -1        \n",
    "        \n",
    "        return is_inlier.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tsfresh').setLevel(logging.ERROR)\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = Path('./', 'data')\n",
    "random_state = 1\n",
    "events_names = {0: 'Normal',\n",
    "                1: 'Aumento Abrupto de BSW',\n",
    "                2: 'Fechamento Espúrio de DHSV',\n",
    "                3: 'Intermitência Severa',\n",
    "                4: 'Instabilidade de Fluxo',\n",
    "                5: 'Perda Rápida de Produtividade',\n",
    "                6: 'Restrição Rápida em CKP',\n",
    "                7: 'Incrustação em CKP',\n",
    "                8: 'Hidrato em Linha de Produção'\n",
    "               }\n",
    "vars = ['P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        'P-JUS-CKGL',\n",
    "        'T-JUS-CKGL',\n",
    "        'QGL']\n",
    "columns = ['timestamp'] + vars + ['class'] \n",
    "normal_class_code = 0\n",
    "abnormal_classes_codes = [1, 2, 5, 6, 7, 8]\n",
    "sample_size = 3*60              # Nas observações = segundos\n",
    "min_normal_period_size = 20*60  # Nas observações = segundos\n",
    "split_range = 0.6               # Porcentagem de separação entre treino/teste\n",
    "max_samples_per_period = 15     # limitação por 'segurança'\n",
    "df_fc_p = MinimalFCParameters() # Ver documentação da biblioteca tsfresh - opção: EfficientFCParameters()\n",
    "df_fc_p.pop('sum_values')       # Remove feature inapropriada\n",
    "df_fc_p.pop('length')           # Remove feature inapropriada\n",
    "max_nan_percent = 0.1           # Para seleção de variáveis\n",
    "std_vars_min = 0.01             # Para seleção de variáveis\n",
    "clfs = {}                       # Dicionário para lista de classificadores a serem experimentados\n",
    "disable_progressbar = True      # Para menos saídas no notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def class_and_file_generator(data_path, real=False, simulated=False, drawn=False):\n",
    "    \"\"\"Gerador de lista contendo número da classe e caminho do arquivo de acordo com a fonte da instância.\"\"\"    \n",
    "    for class_path in data_path.iterdir():\n",
    "        if class_path.is_dir():\n",
    "            class_code = int(class_path.stem)\n",
    "            for instance_path in class_path.iterdir():\n",
    "                if (instance_path.suffix == '.csv'):\n",
    "                    if (simulated and instance_path.stem.startswith('SIMULATED')) or \\\n",
    "                       (drawn and instance_path.stem.startswith('DRAWN')) or \\\n",
    "                       (real and (not instance_path.stem.startswith('SIMULATED')) and \\\n",
    "                       (not instance_path.stem.startswith('DRAWN'))):\n",
    "                        yield class_code, instance_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_instance(instance_path):\n",
    "    \"\"\"Função que carrega cada instância individualmente\"\"\"\n",
    "    try:\n",
    "        well, instance_id = instance_path.stem.split('_')\n",
    "        df = pd.read_csv(instance_path, sep=',', header=0)\n",
    "        assert (df.columns == columns).all(), \\\n",
    "            f'Colunas inválidas no arquivo {str(instance_path)}: {str(df.columns.tolist())}'\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f'Erro ao ler arquivo {instance_path}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_samples(df, class_code):\n",
    "    # Obtém os rótulos das observações e seu conjunto inequívoco\n",
    "    ols = list(df['class'])\n",
    "    set_ols = set()\n",
    "    for ol in ols:\n",
    "        if ol in set_ols or np.isnan(ol):\n",
    "            continue\n",
    "        set_ols.add(int(ol))       \n",
    "    \n",
    "    # Descarta os rótulos das observações e substitui todos os nan por 0\n",
    "    # (requisito da biblioteca tsfresh)\n",
    "    df_vars = df.drop('class', axis=1).fillna(0)  \n",
    "    \n",
    "    # Inicializa objetos que serão retornados\n",
    "    df_samples_train = pd.DataFrame()\n",
    "    df_samples_test = pd.DataFrame()\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "            \n",
    "    # Descubre o número máximo de amostras em períodos normais, transitórios e em regime\n",
    "    # Obtém índices (primeiro e último) sem sobreposição com outros períodos\n",
    "    f_idx = ols.index(normal_class_code)\n",
    "    l_idx = len(ols)-1-ols[::-1].index(normal_class_code)\n",
    "\n",
    "    # Define o número inicial de amostras para o período normal\n",
    "    max_samples_normal = l_idx-f_idx+1-sample_size\n",
    "    if (max_samples_normal) > 0:      \n",
    "        num_normal_samples = min(max_samples_per_period, max_samples_normal)\n",
    "        num_train_samples = int(split_range*num_normal_samples)\n",
    "        num_test_samples = num_normal_samples - num_train_samples    \n",
    "    else:\n",
    "        num_train_samples = 0\n",
    "        num_test_samples = 0\n",
    "    \n",
    "    # Define o número máximo de amostras por período transitório\n",
    "    transient_code = class_code + 100    \n",
    "    if transient_code in set_ols:\n",
    "        # Obtém índices (primeiro e último) com possível sobreposição\n",
    "        # no início do período\n",
    "        f_idx = ols.index(transient_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(transient_code)        \n",
    "        max_transient_samples = l_idx-f_idx+1-sample_size\n",
    "    else:\n",
    "        max_transient_samples = 0            \n",
    "\n",
    "    # Define o número máximo de amostras no período de regime\n",
    "    if class_code in set_ols:\n",
    "        # Obtém índices (primeiro e último) com possível sobreposição \n",
    "        # no início ou fim do período\n",
    "        f_idx = ols.index(class_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(class_code)\n",
    "        if l_idx+(sample_size-1) < len(ols)-1:\n",
    "            l_idx = l_idx+(sample_size-1) \n",
    "        else:\n",
    "            l_idx = len(ols)-1\n",
    "        max_in_regime_samples = l_idx-f_idx+1-sample_size\n",
    "    else:\n",
    "        max_in_regime_samples = 0   \n",
    "        \n",
    "    # Descubre o número adequado de amostras em períodos normais, transitórios e em regime\n",
    "    num_transient_samples = ceil(num_test_samples/2)\n",
    "    num_in_regime_samples = num_test_samples - num_transient_samples\n",
    "    if (max_transient_samples >= num_transient_samples) and \\\n",
    "       (max_in_regime_samples < num_in_regime_samples):\n",
    "        num_in_regime_samples = max_in_regime_samples        \n",
    "        num_transient_samples = min(num_test_samples-num_in_regime_samples, max_transient_samples)\n",
    "    elif (max_transient_samples < num_transient_samples) and \\\n",
    "         (max_in_regime_samples >= num_in_regime_samples):\n",
    "        num_transient_samples = max_transient_samples        \n",
    "        num_in_regime_samples = min(num_test_samples-num_transient_samples, max_in_regime_samples)\n",
    "    elif (max_transient_samples < num_transient_samples) and \\\n",
    "         (max_in_regime_samples < num_in_regime_samples):\n",
    "        num_transient_samples = max_transient_samples\n",
    "        num_in_regime_samples = max_in_regime_samples\n",
    "        num_test_samples = num_transient_samples+num_in_regime_samples\n",
    "    \n",
    "    # Extrai amostras do período normal para treinamento e teste\n",
    "    # Obtém índices (primeiro e último) sem sobreposição com outros períodos\n",
    "    f_idx = ols.index(normal_class_code)\n",
    "    l_idx = len(ols)-1-ols[::-1].index(normal_class_code)\n",
    "    \n",
    "    # Define a etapa correta e extrai amostras\n",
    "    if (num_normal_samples) > 0:  \n",
    "        if num_normal_samples == max_samples_normal:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_samples_normal-1) // (max_samples_per_period-1)\n",
    "        step_wanted = sample_size\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Extrai amostras para treinamento\n",
    "        sample_id = 0\n",
    "        for idx in range(num_train_samples):\n",
    "            f_idx_c = l_idx-sample_size+1-(num_normal_samples-1-idx)*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_train = df_samples_train.append(df_sample)\n",
    "            y_train.append(normal_class_code)\n",
    "            sample_id += 1\n",
    "    \n",
    "        # Extrai amostras para teste\n",
    "        sample_id = 0\n",
    "        for idx in range(num_train_samples, num_train_samples+num_test_samples):\n",
    "            f_idx_c = l_idx-sample_size+1-(num_normal_samples-1-idx)*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(normal_class_code)\n",
    "            sample_id += 1\n",
    "\n",
    "    # Extrai amostras do período transitório (se existir) para teste\n",
    "    if (num_transient_samples) > 0:    \n",
    "        # Define a etapa correta e extrai amostras\n",
    "        if num_transient_samples == max_transient_samples:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_transient_samples-1) // (max_samples_per_period-1)\n",
    "        step_wanted = np.inf\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Obtém índices (primeiro e último) com possível sobreposição no início deste período\n",
    "        f_idx = ols.index(transient_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(transient_code) \n",
    "\n",
    "        # Extrai amostras\n",
    "        for idx in range(num_transient_samples):\n",
    "            f_idx_c = f_idx+idx*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(transient_code)\n",
    "            sample_id += 1\n",
    "            \n",
    "    # Extrai amostras do período em regime (se existir) para teste\n",
    "    if (num_in_regime_samples) > 0:     \n",
    "        # Define a etapa correta e extrai amostras\n",
    "        if num_in_regime_samples == max_in_regime_samples:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_in_regime_samples-1) // (max_samples_per_period-1)\n",
    "        step_wanted = sample_size\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Obtém índices (primeiro e último) com possível sobreposição \n",
    "        # no início ou no final deste período\n",
    "        f_idx = ols.index(class_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(class_code)\n",
    "        if l_idx+(sample_size-1) < len(ols)-1:\n",
    "            l_idx = l_idx+(sample_size-1) \n",
    "        else:\n",
    "            l_idx = len(ols)-1\n",
    "\n",
    "        # Extrai amostras\n",
    "        for idx in range(num_in_regime_samples):\n",
    "            f_idx_c = f_idx+idx*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(class_code)\n",
    "            sample_id += 1\n",
    "            \n",
    "    #print(f'df_samples_train (antes de normalizar):')\n",
    "    #display(df_samples_train)\n",
    "    #print(f'y_train (antes de ajustar para +1 e -1): {y_train} \\n')\n",
    "    #print(f'df_samples_test (antes de normalizar):')\n",
    "    #display(df_samples_test)\n",
    "    #print(f'y_test (antes de ajustar para +1 e -1): {y_test} \\n')\n",
    "    \n",
    "    return df_samples_train, y_train, df_samples_test, y_test              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test_calc_scores(X_train, y_train, X_test, y_test, scores, clfs):\n",
    "    X_train.reset_index(inplace=True, drop=True)\n",
    "    X_test.reset_index(inplace=True, drop=True)    \n",
    "    for clf_name, clf in clfs.items():\n",
    "        #print(f'CLASSIFICADOR: {clf_name}')\n",
    "        #print(f'y_train: {y_train}')\n",
    "        #print(f'y_test: {y_test}')\n",
    "        \n",
    "        # Treino\n",
    "        t0 = time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        t_train = time() - t0\n",
    "\n",
    "        # Teste\n",
    "        t0 = time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        #print(f'y_pred: {y_pred}')\n",
    "        t_test = time() - t0\n",
    "\n",
    "        # Plota os labels reais e preditos pelo classificador\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(12,1))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(-(y_pred), marker=11, color='orange', linestyle='') \n",
    "        plt.plot(-(y_test), marker=10, color='green', linestyle='')  \n",
    "        ax.grid(False)\n",
    "        ax.set_yticks([-1, 1])\n",
    "        ax.set_yticklabels(['Normal', 'Anormal'])\n",
    "        ax.set_title(clf_name)            \n",
    "        ax.set_xlabel('Amostra')\n",
    "        ax.legend(['Classe Prevista', 'Classe Real'])\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calcula as metricas de desempenho\n",
    "        ret = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "        p, r, f1, _ = ret\n",
    "        scores = scores.append({'CLASSIFICADOR': clf_name, \n",
    "                                'PRECISAO': p,\n",
    "                                'REVOGACAO': r,\n",
    "                                'F1': f1,\n",
    "                                'TREINAMENTO [s]': t_train, \n",
    "                                'TESTE [s] ': t_test}, ignore_index=True)  \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gets all real instances but maintains only those with any type of undesirable event\n",
    "real_instances = pd.DataFrame(class_and_file_generator(data_path, \n",
    "                                                       real=True,\n",
    "                                                       simulated=False, \n",
    "                                                       drawn=False),\n",
    "                              columns=['class_code', 'instance_path'])\n",
    "real_instances = real_instances.loc[real_instances.iloc[:,0].isin(abnormal_classes_codes)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________\n",
      "Instância 1: data\\1\\WELL-00001_20140124213136.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (959)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 2: data\\1\\WELL-00002_20140126200050.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (1138)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 3: data\\1\\WELL-00006_20170801063614.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 4: data\\1\\WELL-00006_20170802123000.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 5: data\\1\\WELL-00006_20180618060245.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 6: data\\2\\WELL-00002_20131104014101.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 7: data\\2\\WELL-00003_20141122214325.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 8: data\\2\\WELL-00003_20170728150240.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 9: data\\2\\WELL-00003_20180206182917.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (586)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 10: data\\2\\WELL-00009_20170313160804.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 11: data\\2\\WELL-00010_20171218200131.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 12: data\\2\\WELL-00011_20140515110134.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 13: data\\2\\WELL-00011_20140530100015.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (482)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 14: data\\2\\WELL-00011_20140606230115.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 15: data\\2\\WELL-00011_20140720120102.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 16: data\\2\\WELL-00011_20140726180015.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (900)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 17: data\\2\\WELL-00011_20140824000118.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 18: data\\2\\WELL-00011_20140916060300.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 19: data\\2\\WELL-00011_20140921200031.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (695)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 20: data\\2\\WELL-00011_20140928100056.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 21: data\\2\\WELL-00011_20140929170028.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (975)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 22: data\\2\\WELL-00011_20140929220121.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 23: data\\2\\WELL-00011_20141005170056.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 24: data\\2\\WELL-00011_20141006160121.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 25: data\\2\\WELL-00012_20170320033022.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (773)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 26: data\\2\\WELL-00012_20170320143144.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 27: data\\2\\WELL-00013_20170329020229.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 28: data\\5\\WELL-00015_20170620160349.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 29: data\\5\\WELL-00015_20171013140047.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 30: data\\5\\WELL-00016_20180405020345.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (1145)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 31: data\\5\\WELL-00016_20180426142005.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (321)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 32: data\\5\\WELL-00016_20180426145108.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (376)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 33: data\\5\\WELL-00016_20180517222322.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 34: data\\5\\WELL-00017_20140314180000.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (0)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 35: data\\5\\WELL-00017_20140317151743.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 36: data\\5\\WELL-00017_20140318023141.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 37: data\\5\\WELL-00017_20140318160220.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 38: data\\5\\WELL-00017_20140319040453.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 39: data\\5\\WELL-00017_20140319141450.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 40: data\\6\\WELL-00002_20140212170333.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 41: data\\6\\WELL-00002_20140301151700.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 42: data\\6\\WELL-00002_20140325170304.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 43: data\\6\\WELL-00004_20171031181509.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 44: data\\6\\WELL-00004_20171031193025.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (414)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 45: data\\6\\WELL-00004_20171031200059.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (845)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 46: data\\7\\WELL-00001_20170226220309.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 47: data\\7\\WELL-00006_20180618110721.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 48: data\\7\\WELL-00006_20180620181348.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 49: data\\7\\WELL-00018_20180611040207.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (0)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 50: data\\8\\WELL-00019_20170301182317.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 51: data\\8\\WELL-00020_20120410192326.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (173)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 52: data\\8\\WELL-00021_20170509013517.csv\n"
     ]
    }
   ],
   "source": [
    "# For each real instance with any type of undesirable event\n",
    "scores = pd.DataFrame()\n",
    "ignored_instances = 0\n",
    "used_instances = 0\n",
    "for i, row in real_instances.iterrows():\n",
    "    # Loads the current instance\n",
    "    class_code, instance_path = row\n",
    "    print(f'____________________________________________________________________________________')        \n",
    "    print(f'Instância {i+1}: {instance_path}')\n",
    "    df = load_instance(instance_path)\n",
    "    \n",
    "    # Ignores instances without sufficient normal periods\n",
    "    normal_period_size = (df['class']==float(normal_class_code)).sum()\n",
    "    if normal_period_size < min_normal_period_size:\n",
    "        ignored_instances += 1\n",
    "        print(f'\\tignorado porque normal_period_size é insuficiente para treinamento ({normal_period_size})\\n')\n",
    "        continue\n",
    "    used_instances += 1\n",
    "        \n",
    "    # Extracts samples from the current real instance\n",
    "    df_samples_train, y_train, df_samples_test, y_test = extract_samples(df, class_code)\n",
    "\n",
    "    # Changes types of the labels (tsfresh's requirement)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # (Wander) incluidas as duas linhas abaixo para ajustar y_train também (?)\n",
    "    y_train[y_train!=normal_class_code] = -1\n",
    "    y_train[y_train==normal_class_code] = 1\n",
    "\n",
    "    # We want binary classification: 1 for inliers (negative class = normal instance) and\n",
    "    # -1 for outliers (positive class = instance with anomaly) (sklearn's requirement)\n",
    "    y_test[y_test!=normal_class_code] = -1\n",
    "    y_test[y_test==normal_class_code] = 1\n",
    "    \n",
    "    # Drops the bad vars\n",
    "    good_vars = np.isnan(df_samples_train[vars]).mean(0) <= max_nan_percent\n",
    "    std_vars = np.nanstd(df_samples_train[vars], 0)\n",
    "    good_vars &= (std_vars > std_vars_min)    \n",
    "    good_vars = list(good_vars.index[good_vars])\n",
    "    bad_vars = list(set(vars)-set(good_vars))\n",
    "    df_samples_train.drop(columns=bad_vars, inplace=True, errors='ignore')\n",
    "    df_samples_test.drop(columns=bad_vars, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Normalizes the samples (zero mean and unit variance)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    df_samples_train[good_vars] = scaler.fit_transform(df_samples_train[good_vars]).astype('float32')\n",
    "    df_samples_test[good_vars] = scaler.transform(df_samples_test[good_vars]).astype('float32')\n",
    "    \n",
    "    # Extracts features from samples\n",
    "    X_train = extract_features(df_samples_train, \n",
    "                               column_id='id', \n",
    "                               column_sort='timestamp', \n",
    "                               default_fc_parameters=df_fc_p,\n",
    "                               impute_function=impute,\n",
    "                               n_jobs=0,\n",
    "                               disable_progressbar=disable_progressbar)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = extract_features(df_samples_test, \n",
    "                              column_id='id', \n",
    "                              column_sort='timestamp',\n",
    "                              default_fc_parameters=df_fc_p,\n",
    "                              impute_function=impute,\n",
    "                              n_jobs=0,\n",
    "                              disable_progressbar=disable_progressbar)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    \n",
    "    #print(f'df_samples_train (normalizado e tratado):')\n",
    "    #display(df_samples_train)\n",
    "    #print(f'df_samples_test (normalizado):')\n",
    "    #display(df_samples_test)\n",
    "    #print(f'X_train:')\n",
    "    #display(X_train)\n",
    "    #print(f'y_train (ajustado para +1 e -1): {y_train} \\n')\n",
    "    #print(f'X_test:')\n",
    "    #display(X_test)\n",
    "    #print(f'y_test (ajustado para +1 e -1): {y_test} \\n')\n",
    "\n",
    "    # LISTA DE CLASSIFICADORES A SEREM EXPERIMENTADOS\n",
    "        \n",
    "    # DUMMY - Classificador ingênuo\n",
    "    clfs['Dummy'] = DummyClassifier(strategy='constant', constant=1)\n",
    "    \n",
    "    # ISOLATION FOREST - busca de melhores hiperparâmetros (384 combinações)\n",
    "    isolation_forest_params = {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_samples': ['auto', 0.50, 0.75, 1.0],\n",
    "        'contamination': ['auto', 0, 0.05, 0.10],\n",
    "        'bootstrap': [True, False],\n",
    "        'max_features': [0.50, 0.75, 1.0],\n",
    "        'random_state': [random_state]\n",
    "    }\n",
    "    for params in ParameterGrid(isolation_forest_params):\n",
    "        isolation_forest_clf = IsolationForest().set_params(**params)\n",
    "        clfs[f'Floresta de Isolamento: {params}'] = isolation_forest_clf\n",
    "\n",
    "    # ONE CLASS SVM - busca de melhores hiperparâmetros (240 combinações)\n",
    "    ocsvm_params = {\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['auto', 'scale', 1e-4, 1e-3, 1e-2, 0.1, 0.50, 1.0, 5.0, 10.0],\n",
    "        'nu': [1e-4, 1e-3, 1e-2, 0.10, 0.50, 1.0]\n",
    "    }    \n",
    "    for params in ParameterGrid(ocsvm_params):\n",
    "        ocsvm_clf = OneClassSVM().set_params(**params)\n",
    "        clfs[f'One-Class SVM: {params}'] = ocsvm_clf\n",
    "    \n",
    "    # LOCAL OUTLIER FACTOR (LOF) - busca de melhores hiperparâmetros (1056 combinações)\n",
    "    lof_params = {\n",
    "        'n_neighbors': [5, 10, 15, 20],\n",
    "        'algorithm': ['auto'],\n",
    "        'leaf_size': [15, 30, 45], # Leaf size passed to BallTree or KDTree algorithm. \n",
    "        'metric': ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'braycurtis', \n",
    "                   'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', \n",
    "                   'kulsinski', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', \n",
    "                   'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'],\n",
    "        'contamination': ['auto', 0.01, 0.05, 0.10],\n",
    "        'novelty': [True]\n",
    "    }    \n",
    "    for params in ParameterGrid(lof_params):\n",
    "        lof_clf = LocalOutlierFactor().set_params(**params)\n",
    "        clfs[f'Local Outlier Factor (LOF): {params}'] = lof_clf\n",
    "    \n",
    "    # ELLIPTIC ENVELOPE(LOF) - busca de melhores hiperparâmetros (36 combinações)\n",
    "    ellipticenvelope_params = {\n",
    "        'contamination': [1e-4, 1e-3, 0.01, 0.05, 0.10, 0.50],\n",
    "        'assume_centered': [True, False],\n",
    "        'support_fraction': [0.80, 0.90, 0.99], # proportion of points to be included in the support of the raw MCD estimate. \n",
    "        'random_state': [random_state]\n",
    "    }    \n",
    "    for params in ParameterGrid(ellipticenvelope_params):\n",
    "        ellipticenvelope_clf = EllipticEnvelope().set_params(**params)\n",
    "        clfs[f'Envelope Elíptico: {params}'] = ellipticenvelope_clf\n",
    "    \n",
    "\n",
    "    # Preenchido abaixo somente apos encontrar os melhores parametros\n",
    "    clfs_only_best = {'One Class SVM':     OneClassSVM(kernel='rbf', gamma=0.001, nu=0.1),\n",
    "            'Floresta de Isolamento':        IsolationForest(bootstrap=False, contamination=0, \n",
    "                                                             max_features=1.0, max_samples=1.0, \n",
    "                                                             n_estimators=150, random_state=1),\n",
    "            'Envelope Eliptico': EllipticEnvelope(assume_centered=False, contamination=0.0001, \n",
    "                                                  random_state=1, support_fraction=0.8),\n",
    "            'Local Outlier Factor': LocalOutlierFactor(algorithm='auto', contamination='auto', \n",
    "                                                       leaf_size=15, metric='chebyshev', \n",
    "                                                       n_neighbors=10, novelty=True),\n",
    "            'AutoEncoder': AutoEncoder(hidden_neurons=[8, 4, 4, 8], hidden_activation='relu', \n",
    "                                               output_activation='sigmoid', loss=keras.losses.mean_squared_error, \n",
    "                                               optimizer='adam', epochs=20, batch_size=2, dropout_rate=0.2, \n",
    "                                               l2_regularizer=0.1, validation_size=0.1, \n",
    "                                               preprocessing=True, verbose=0, \n",
    "                                               random_state=1, contamination=0.1),\n",
    "            'Dummy': DummyClassifier(strategy='constant', constant=1)\n",
    "            } \n",
    "    \n",
    "    # Treina, testa e calcula os scores para cada classificador na instância    \n",
    "    \n",
    "    # Para todas as combinações (usando clfs):\n",
    "    # scores = train_test_calc_scores(X_train, y_train, X_test, y_test, scores, clfs)\n",
    "    \n",
    "    # Somente para a melhor combinação de cada um (usando clfs_only_best):\n",
    "    scores = train_test_calc_scores(X_train, y_train, X_test, y_test, scores, clfs_only_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados obtidos com os métodos implementados são apresentados abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de instâncias utilizadas: 36\n",
      "Número de instâncias ignoradas: 16\n"
     ]
    }
   ],
   "source": [
    "print(f'Número de instâncias utilizadas: {used_instances}')\n",
    "print(f'Número de instâncias ignoradas: {ignored_instances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características utilizadas: ['median', 'mean', 'standard_deviation', 'variance', 'maximum', 'minimum']\n"
     ]
    }
   ],
   "source": [
    "print(f'Características utilizadas: {list(df_fc_p.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os comandos a seguir permitem salvar e recuperar os resultados de/para um arquivo CSV de forma conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.to_csv(r'./results/anomaly_detection_scores_por_rodada.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Métricas em formato tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As tabelas a seguir apresentam as médias e o desvio padrão das métricas, respectivamente. Ambos são ordenados pela medida-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>PRECISAO</th>\n",
       "      <th>REVOGACAO</th>\n",
       "      <th>TESTE [s]</th>\n",
       "      <th>TREINAMENTO [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIFICADOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Local Outlier Factor</th>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.001841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Floresta de Isolamento</th>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.126486</td>\n",
       "      <td>0.487763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Envelope Eliptico</th>\n",
       "      <td>0.664352</td>\n",
       "      <td>0.664352</td>\n",
       "      <td>0.664352</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.051549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One Class SVM</th>\n",
       "      <td>0.567130</td>\n",
       "      <td>0.567130</td>\n",
       "      <td>0.567130</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.001718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AutoEncoder</th>\n",
       "      <td>0.439815</td>\n",
       "      <td>0.439815</td>\n",
       "      <td>0.439815</td>\n",
       "      <td>0.008975</td>\n",
       "      <td>4.241497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              F1  PRECISAO  REVOGACAO  TESTE [s]   \\\n",
       "CLASSIFICADOR                                                       \n",
       "Local Outlier Factor    0.881944  0.881944   0.881944    0.002051   \n",
       "Floresta de Isolamento  0.743056  0.743056   0.743056    0.126486   \n",
       "Envelope Eliptico       0.664352  0.664352   0.664352    0.001673   \n",
       "One Class SVM           0.567130  0.567130   0.567130    0.001283   \n",
       "Dummy                   0.500000  0.500000   0.500000    0.000192   \n",
       "AutoEncoder             0.439815  0.439815   0.439815    0.008975   \n",
       "\n",
       "                        TREINAMENTO [s]  \n",
       "CLASSIFICADOR                            \n",
       "Local Outlier Factor           0.001841  \n",
       "Floresta de Isolamento         0.487763  \n",
       "Envelope Eliptico              0.051549  \n",
       "One Class SVM                  0.001718  \n",
       "Dummy                          0.000273  \n",
       "AutoEncoder                    4.241497  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Médias\n",
    "mean_score_table = scores.groupby('CLASSIFICADOR').mean().sort_values(by=['F1'], ascending=False)\n",
    "mean_score_table.to_csv(r'./results/anomaly_detection_scores_medias.csv')\n",
    "mean_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>PRECISAO</th>\n",
       "      <th>REVOGACAO</th>\n",
       "      <th>TESTE [s]</th>\n",
       "      <th>TREINAMENTO [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIFICADOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Local Outlier Factor</th>\n",
       "      <td>0.126577</td>\n",
       "      <td>0.126577</td>\n",
       "      <td>0.126577</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AutoEncoder</th>\n",
       "      <td>0.137453</td>\n",
       "      <td>0.137453</td>\n",
       "      <td>0.137453</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.845468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Envelope Eliptico</th>\n",
       "      <td>0.157468</td>\n",
       "      <td>0.157468</td>\n",
       "      <td>0.157468</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.011521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One Class SVM</th>\n",
       "      <td>0.162836</td>\n",
       "      <td>0.162836</td>\n",
       "      <td>0.162836</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Floresta de Isolamento</th>\n",
       "      <td>0.179699</td>\n",
       "      <td>0.179699</td>\n",
       "      <td>0.179699</td>\n",
       "      <td>0.028920</td>\n",
       "      <td>0.098157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              F1  PRECISAO  REVOGACAO  TESTE [s]   \\\n",
       "CLASSIFICADOR                                                       \n",
       "Dummy                   0.000000  0.000000   0.000000    0.000397   \n",
       "Local Outlier Factor    0.126577  0.126577   0.126577    0.000771   \n",
       "AutoEncoder             0.137453  0.137453   0.137453    0.002714   \n",
       "Envelope Eliptico       0.157468  0.157468   0.157468    0.000592   \n",
       "One Class SVM           0.162836  0.162836   0.162836    0.000656   \n",
       "Floresta de Isolamento  0.179699  0.179699   0.179699    0.028920   \n",
       "\n",
       "                        TREINAMENTO [s]  \n",
       "CLASSIFICADOR                            \n",
       "Dummy                          0.000461  \n",
       "Local Outlier Factor           0.000640  \n",
       "AutoEncoder                    0.845468  \n",
       "Envelope Eliptico              0.011521  \n",
       "One Class SVM                  0.000631  \n",
       "Floresta de Isolamento         0.098157  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desvios Padrão\n",
    "std_score_table = scores.groupby('CLASSIFICADOR').std().sort_values(by=['F1'], ascending=True)\n",
    "std_score_table.to_csv(r'./results/anomaly_detection_scores_desvios_padrao.csv')\n",
    "std_score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Testes estatísticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizado teste de Friedman e teste de Holm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value: 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "# Teste Estatístico (Friedman)\n",
    "\n",
    "clfs_names = list(clfs_only_best.keys())\n",
    "f1s = [scores.loc[scores['CLASSIFICADOR']==cn, 'F1'].values for cn in clfs_names]\n",
    "f_value_stat, p_value, ranks, pivots = stac.friedman_test(*(f1s))\n",
    "print(f'p_value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy vs Local Outlier Factor: \n",
      "\tp_values: 3.4439118223872356e-13\n",
      "\tadj_p_values: 1.7219559111936178e-12\n",
      "Dummy vs Floresta de Isolamento: \n",
      "\tp_values: 4.254053665997404e-06\n",
      "\tadj_p_values: 1.7016214663989615e-05\n",
      "Dummy vs Envelope Eliptico: \n",
      "\tp_values: 0.0008417560142277569\n",
      "\tadj_p_values: 0.0025252680426832708\n",
      "Dummy vs AutoEncoder: \n",
      "\tp_values: 0.2313504077741384\n",
      "\tadj_p_values: 0.4627008155482768\n",
      "Dummy vs One Class SVM: \n",
      "\tp_values: 0.27028938480169806\n",
      "\tadj_p_values: 0.4627008155482768\n"
     ]
    }
   ],
   "source": [
    "# Teste Estatístico (Holm)\n",
    "\n",
    "comp, z_values_stat, p_values, adj_p_values = stac.holm_test(len(pivots), pivots, clfs_names, clfs_names.index('Dummy'))\n",
    "for i in range(len(comp)):\n",
    "    print(f'{comp[i]}: \\n\\tp_values: {p_values[i]}\\n\\tadj_p_values: {adj_p_values[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execução (hh:mm:ss.ms): 0:03:24.636026\n"
     ]
    }
   ],
   "source": [
    "# Calcular tempo total do notebook Jupyter\n",
    "print(f'Tempo total de execução (hh:mm:ss.ms): {datetime.now() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Sumário",
   "title_sidebar": "Sumário",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
